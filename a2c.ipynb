{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.distributions import Categorical\n",
    "import gym\n",
    "from connect4env import *\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy and value model\n",
    "class ActorCriticNet(nn.Module):\n",
    "  def __init__(self, obs_space_size, action_space_size):\n",
    "    super().__init__()\n",
    "\n",
    "    self.shared_layers = nn.Sequential(\n",
    "        nn.Linear(obs_space_size, 1024),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(1024, 512),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(512, 256),\n",
    "        nn.ReLU())\n",
    "    \n",
    "    self.policy_layers = nn.Sequential(\n",
    "        nn.Linear(256, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, action_space_size))\n",
    "    \n",
    "    self.value_layers = nn.Sequential(\n",
    "        nn.Linear(256, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, 1))\n",
    "    \n",
    "  def value(self, obs):\n",
    "    z = self.shared_layers(obs)\n",
    "    value = self.value_layers(z)\n",
    "    return value\n",
    "        \n",
    "  def policy(self, obs):\n",
    "    z = self.shared_layers(obs)\n",
    "    policy_logits = self.policy_layers(z)\n",
    "    return policy_logits\n",
    "\n",
    "  def forward(self, obs):\n",
    "    z = self.shared_layers(obs)\n",
    "    policy_logits = self.policy_layers(z)\n",
    "    value = self.value_layers(z)\n",
    "    return policy_logits, value\n",
    "  \n",
    "# Define the Actor-Critic Trainer\n",
    "class ActorCriticTrainer:\n",
    "    def __init__(self, model, lr=1e-3):\n",
    "        self.model = model.to(device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "\n",
    "    def update(self, obs, acts, returns, values, advantages):\n",
    "        self.optimizer.zero_grad()\n",
    "        policy_logits = self.model.policy(obs.to(device))\n",
    "    \n",
    "        policy_dist = Categorical(logits=policy_logits)\n",
    "        log_probs = policy_dist.log_prob(acts)\n",
    "        policy_loss = -(log_probs * advantages).mean()\n",
    "        \n",
    "        value_loss = nn.MSELoss()(returns, values).mean()\n",
    "        # value_loss = nn.SmoothL1Loss()(returns, value)\n",
    "        # entropy_loss = (policy_dist.entropy()).mean()\n",
    "        loss = policy_loss + value_loss \n",
    "        \n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "    def save_model(self):\n",
    "        torch.save(self.model, \"a2c_net5.pth\")\n",
    "        \n",
    "\n",
    "    def load_model(self):\n",
    "        self.model = torch.load(\"a2c_net3.pth\").to(self.device)\n",
    "        \n",
    "\n",
    "def calculate_advantages(rewards, values, gamma=0.99, lambda_=0.95):\n",
    "    advantages = []\n",
    "    advantage = 0\n",
    "    for t in reversed(range(len(rewards))):\n",
    "        delta = rewards[t] - values[t]\n",
    "        advantage = delta + gamma * lambda_ * advantage\n",
    "        advantages.append(advantage)\n",
    "    advantages.reverse()\n",
    "    advantages = torch.tensor(advantages, dtype=torch.float32)\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "    return advantages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "# Initialize the environment and model\n",
    "env = Connect4Env()\n",
    "model = ActorCriticNet(reduce(lambda x,y: x*y, env.observation_space.shape), env.action_space.n)\n",
    "model.to(device)\n",
    "trainer = ActorCriticTrainer(model)\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "num_episodes = 250000\n",
    "gamma = 0.99\n",
    "print_freq = 100\n",
    "rewards = []\n",
    "loss = []\n",
    "writer = SummaryWriter()\n",
    "for episode in range(num_episodes):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    episode_rewards = []\n",
    "    episode_values = []\n",
    "    # model.zero_grad()\n",
    "    while not done:\n",
    "        obs_tensor = torch.tensor([obs.flatten()], dtype=torch.float32)\n",
    "        obs_tensor = obs_tensor.to(device)\n",
    "        policy_logits, value = model(obs_tensor)\n",
    "        valid_actions = env.get_valid_action_mask()\n",
    "        action_mask = torch.from_numpy(valid_actions.flatten()).to(device)\n",
    "        action_masked = torch.where(action_mask, policy_logits, -float('inf'))\n",
    "        action_probs = torch.softmax(action_masked.squeeze(0), dim=0)\n",
    "        # Select a valid action\n",
    "        valid_action_indices = torch.nonzero(action_mask.flatten()).squeeze(1)\n",
    "        action_index = torch.multinomial(action_probs[valid_action_indices], num_samples=1).item()\n",
    "        action = valid_action_indices[action_index].item()\n",
    "        # action = Categorical(logits=policy_logits).sample().item()\n",
    "\n",
    "        next_obs, reward, done, _ = env.step(action)\n",
    "        episode_rewards.append(reward)\n",
    "        episode_values.append(value.item())\n",
    "\n",
    "        obs = next_obs\n",
    "    rewards.append(sum(episode_rewards))\n",
    "    # Calculate returns and advantages\n",
    "    returns = []\n",
    "    advantages = []\n",
    "    R = 0\n",
    "    for r, v in zip(reversed(episode_rewards), reversed(episode_values)):\n",
    "        R = r + gamma * R\n",
    "        returns.insert(0, R)\n",
    "        advantages.insert(0, R - v)\n",
    "\n",
    "    # Normalize returns\n",
    "    returns = torch.tensor(returns, dtype=torch.float32).to(device)\n",
    "    returns = (returns - returns.mean()) / (returns.std() + 1e-8).to(device)\n",
    "    \n",
    "    # advantages = torch.tensor(advantages, dtype=torch.float32)\n",
    "    # advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "    advantages = calculate_advantages(episode_rewards, episode_values).to(device)\n",
    "    values = torch.tensor(episode_values, dtype=torch.float32).to(device)\n",
    "    # values = (values - values.mean()) / (values.std() + 1e-8)\n",
    "\n",
    "    obs_tensor = torch.tensor([obs.flatten()], dtype=torch.float32).to(device)\n",
    "    ep_loss = trainer.update(obs_tensor, torch.tensor([action]).to(device), returns, values, advantages)\n",
    "    loss.append(ep_loss)\n",
    "    # Print episode info\n",
    "    if (episode + 1) % print_freq == 0:\n",
    "        trainer.save_model()\n",
    "        writer.add_scalar('Average Reward', np.mean(rewards[-print_freq:]), episode+1)\n",
    "        writer.add_scalar('Average Loss', np.mean(loss[-print_freq:]), episode+1)\n",
    "    \n",
    "        print(f\"Episode: {episode+1}, Avg Reward: {np.mean(rewards[-print_freq:])}, Loss: {np.mean(loss[-print_freq:])}\")\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "PATH = \"/home/anand/OnitamaRL/Weights/a2c_net5.pth\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "PATH2 = \"/home/anand/OnitamaRL/Weights/model_ep234999_reward-0.3_action_mask.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 73.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 | Steps 4 | Reward 1.0 | Wins P1 1 | Wins P2 0\n",
      "Episode 2 | Steps 4 | Reward 1.0 | Wins P1 2 | Wins P2 0\n",
      "Episode 3 | Steps 4 | Reward 1.0 | Wins P1 3 | Wins P2 0\n",
      "Episode 4 | Steps 4 | Reward 1.0 | Wins P1 4 | Wins P2 0\n",
      "Episode 5 | Steps 4 | Reward 1.0 | Wins P1 5 | Wins P2 0\n",
      "Episode 6 | Steps 4 | Reward 1.0 | Wins P1 6 | Wins P2 0\n",
      "Episode 7 | Steps 4 | Reward 1.0 | Wins P1 7 | Wins P2 0\n",
      "Episode 8 | Steps 4 | Reward 1.0 | Wins P1 8 | Wins P2 0\n",
      "Episode 9 | Steps 4 | Reward 1.0 | Wins P1 9 | Wins P2 0\n",
      "Episode 10 | Steps 4 | Reward 1.0 | Wins P1 10 | Wins P2 0\n",
      "WINS P1: 10\n",
      "WINS P2: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "from train_a2c import ActorCriticNetwork\n",
    "# from minimax import MinimaxAgent\n",
    "from connect4env import Connect4Env\n",
    "from agent import RandomAgent, A2CAgent, PPOAgent, MinimaxAgent\n",
    "from functools import reduce\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def main():\n",
    "\n",
    "    env = Connect4Env()\n",
    "    a2c_agent = A2CAgent(PATH, env, device)\n",
    "    ppo_agent = PPOAgent(PATH2, env, device)\n",
    "    minimax_agent = MinimaxAgent(depth=3, player=-1)\n",
    "    MINIMAX = False\n",
    "    random_agent = RandomAgent()\n",
    "    VERBOSE = False\n",
    "    print_freq = 1\n",
    "    wins = 0\n",
    "    n_test = 10\n",
    "    loss =0\n",
    "    for episode_idx in tqdm(range(n_test)):\n",
    "        ep_reward = 0\n",
    "        obs, info = env.reset()\n",
    "\n",
    "        for step in range(1000):\n",
    "            action_taken = ppo_agent.get_action(env)\n",
    "            next_obs, reward, done, info = env.step(action_taken)\n",
    "\n",
    "            obs = next_obs\n",
    "            ep_reward += reward\n",
    "\n",
    "            if done:\n",
    "                if env.check_win(verbose=VERBOSE):\n",
    "                    wins += 1\n",
    "                else:\n",
    "                    loss += 1\n",
    "                break\n",
    "\n",
    "            # Player 2\n",
    "            if MINIMAX and step == 0:\n",
    "                # So that the minimax agent doesnt always make the same moves again and again\n",
    "                action = random_agent.get_action(env)\n",
    "            else:\n",
    "                action = a2c_agent.get_action(env)\n",
    "            next_obs, reward, done, info, = env.step(action)\n",
    "\n",
    "            obs = next_obs\n",
    "\n",
    "            if done:\n",
    "                if env.check_win(verbose=VERBOSE):\n",
    "                    loss += 1\n",
    "                else:\n",
    "                    wins += 1\n",
    "                break\n",
    "        \n",
    "        if VERBOSE:\n",
    "            print(obs)\n",
    "\n",
    "        if (episode_idx + 1) % print_freq == 0:\n",
    "            print('Episode {} | Steps {} | Reward {:.1f} | Wins P1 {} | Wins P2 {}'.format(episode_idx + 1, step+1, ep_reward, wins, loss))\n",
    "\n",
    "\n",
    "    print(\"WINS P1: {}\".format(wins))\n",
    "    print(\"WINS P2: {}\".format(loss))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
